# 问题一：选举的多线程问题
选举线程开启多个线程向所有peer发送rpc，在处理requestVote结果时存在多线程同步的问题。

rpc需要一定时间，因此主线程应使用条件变量的wait来等待rpc完成，但等待的时间不能是无限的，本节点有可能和其他节点网络分区，得到的票数永远不可能过半，这样可能会导致主线程一直在wait中等待。有两种思路可以解决这个问题：

- 选举计时器会在一段时间后重新发起一次选举，届时会更新term（增加1），可以在本线程中保存一个term，在wait时将其与公共的currentTerm比较，若发现自己持有的term已经过时，就无条件终止这一次选举，让位给新的选举流程。
- 给rpc设置一定超时时间，若过了这段时间仍然没有收集到足够的选票就结束这次选举，rpc的超时时间依赖于通信，但应是远小于选举超时时间（范围）的。

# 问题二：锁的优化

- 通常来说，一个对象使用一个互斥量来管理自己的多个线程对自己资源的访问，使用互斥量的个数越多，管理起来越复杂，发生死锁的概率越大。
- 在此基础上，优化加锁和解锁在代码中的位置可以解决死锁问题和优化并发程度。
- 可以用共享互斥量替换互斥量，在特定情景下使用读写锁优化并发程度。如：KVServer的底层状态机使用一个map替代，对这个map的多线程操作可以使用读写锁管理
- 对很短的任务，考虑使用自旋锁

# 问题三：持久化

- 联动redis持久化


# 问题四： 快照及其优化

- 制作快照和传播快照时如何处理新的数据操作

- 优化：全量快照和增量快照
    - 需要考虑的是传播快照的过程使用了RPC，也就是说会在网络上传输快照信息，如果每次都传播全量快照会带来很大性能损耗
    - 如何得到正确的增量快照？使用增量快照会带来什么问题？

# 优化点：prevote

- 在原先的选举流程中，candidate先自增term，再向其他节点发送requestVote，如果发生网络分区，节点数未过半的分区内会不断发生重新选举，导致term数会推到比正常工作节点高很多的一个量
    - 这样的结果就是，当这些节点恢复到网络中时，不可能成为leader，但比实际leader的term高出很多。
    - 这些恢复节点会拒绝来自leader的复制日志RPC（因为自己的term更大）。在存在能够正常运行的leader的情况下，这些恢复节点会长期无法正常工作（保存正确的数据副本），即使不存在能正常运行的leader，实际具备leader能力的节点会在整个网络中发起选举，而他也只能在反复发起多轮选举，将term推到和网络分区恢复节点相等时才能获得leader
地位，这会无效拉长选举过程

- 解决办法PreVote：很简单的优化，调换升级为leader和自增term的顺序
    - 先向所有节点发送requestVote，并声称自己的term为term+1，但并不实际自增，同时，也不向自己投票
    - 如果获取了多数人-1（除去自己）的票，继续流程，否则不改变任何状态
    - 获得足够票数后，投票给自己，成为leader，并自增term

# 优化点：no-op

- Raft有一个重要的原则：leader只能通过多数共识提交当前任期的log，不能通过多数共识提交之前任期的log
    - 进一步解释：有两种情况能提交（commit）一条log：1. 一条新产生的log在复制到多数节点后，在leader节点提交 2. 新的log提交成功时会累计提交它之前所有未提交的log
    - 该原则限制leader非当前任期的log只能通过情况2提交，不能通过1
    - 这个原则保护了共识原则，反例见论文Figure 8

- 遵守该原则可能会出现一点性能损耗：
    - 新leader当选后，必须等到新的log产生或下一次心跳，才能继续下面的流程，如果没有新log产生，那么它之前的log就无法得到commit
    - 因此，在新leader当选时立即产生一个当前任期没有任何操作的log，既满足了前面的原则，有不会产生额外延迟


# 问题五：幽灵复现

问题见 https://developer.aliyun.com/group/alitech/article/#/

Raft通过上面的只提交最新term log和投票条件的限制（只投给最新term和最长log的节点）解决了幽灵复现问题

# 优化点：ReadIndex

- 首先思考一种很自然的强一致性读方法：LogRead，读操作虽然不改变数据库内容，但是我们也为其创造一条日志，通过心跳RPC传播到各个follower节点，获得多数共识后再执行读取，将结果返回给用户
    - 读取也需要过共识这一关，这是必要的，一个很简单的反例：当前leader发生了网络分区，实际上失去了leader资格，但在发生超时选举之前它无法得知这件事，此时如果遇到读请求而不去尝试获取共识，就会发生一致性错误，因为此时leader可能另有其人，而且新leader上已经有新的操作了。
    - LogRead实际上也保证了读操作的线性一致性
    - 在此基础上，可以优化获取共识的过程，尽可能避免RPC传输log和log落盘等性能损耗

- ReadIndex优化省去了传播读log的过程，只使用心跳，如果收到过半节点的响应就直接在状态机执行读取，为了保证线性一致，记录收到读请求时的commitIndex，因此该优化称为ReadIndex
    - 收到读请求时记录当前的commitIndex，作为readindex
    - 执行一轮心跳，如果收到过半节点响应，可以确定自己当前的leader身份
    - 直到自己的applyIndex大于等于readindex，就可以执行读操作，将结果返回给客户端
    - 当一个Leader刚上任时，有可能存在之前任期未提交的log，这个现象前面已经详细介绍过，此时直接执行ReadIndex的话，由于没有新log产生，需要等到下一个当前任期新log的产生才能推进读流程，会增加操作延迟，好在前面已经进行过了no-op优化，leader可以在上任后快速进入可ReadIndex状态，这两个优化是相得益彰的。


# 优化点：非Leader读取

- 很容易想到，直接在Follower上执行读取是牺牲一致性换取性能，优化点在于：如何让Follower保证一致性？
    - 默认的行为是Follower将自己得知的Leader节点告诉客户，让客户去问Leader
    - 如果让Follower将请求转发给Leader，代价似乎并没减少，还可能增加并发量
    - 在ReadIndex优化后，Follower可以在本地执行读，因为读并不产生日志，也就不需要和其他节点同步
    - 但仍然需要保证对读操作的共识和线性一致性，这里Follower只需要向leader请求一个readindex就可以了


# 优化点：线程池

- 联动线程池项目


# 问题六：Follower何时重置自己的投票结果

收到任意RPC，若对方term大于自己，则更新投票状态。达到选举超时时间后，先投票给自己，没有成为leader的话才恢复为未投票。


# 关键状态

## 持久化状态

currentTerm
votedFor
logs[]

snapShot
lastSnapShotedLogTerm_;
lastSnapShotedLogIndex_;

## nextIndex

matchedIndex

nextIndex

都是日志复制时使用到的状态，nextIndex代表第i个peer下一个需要同步的日志index，matchedIndex为第i个peer最近一个匹配的日志index，正常情况下matchedIndex=nextIndex-1

### 快速回退

当follower拒绝日志复制RPC时，根据其返回的冲突日志index和term决定下一次应该发送的日志是哪个

- 返回的conflictTerm < 0时，代表follower缺少对应日志，nextIndex设置为返回的conflictIndex
- 否则，寻找和conflictTerm相同的最新一条日志
    - 若没找到，nextIndex就设置为conflictIndex，发送所有冲突日志覆盖之
    - 若找到了，就从紧挨着conflictTerm的下一个term开始，发送所有冲突日志

而follower的行为如下

- 寻找RPC发来的prevlogindex
    - 如果没有这个index的log，返回conflictTerm=-1，conflictIndex=最后一条日志index+1
    - 如果找到了，且term匹配，可以将RPC发来的log复制到本地
        - 如果term不匹配，找到相同term的第一个index，返回冲突的term和找到的index


# 关键RPC流程

## appendEntries

发送端：
- 调用RPC
- 判断term
- 若对方接受了日志复制，更新nextIndex，matchedIndex，若收集到过半确认，更新commitedIndex
    - 否则进入快速回退流程

接收端：
- 判断term
- 判断日志是否匹配，若匹配成功，进入复制阶段
    - 否则进入快速回退

## requestVote

发送端：
- 调用RPC
- 判断term
- 若对方投票，计数加1

接收端：
- 判断term
- 若未投票或恰好投给了RPC发送方，继续
    判断rpc的term比自己更大或相等且对方的log更长则投票给对方


# KVServer和KVClerk工作流程

Clerk和Server之间通过RPC交流

Clerk的关键点
- 发送请求之前首先自增requestId，向recentLeader发送请求
- 接受到请求后，如果不是leader，换下一个节点
    - 是leader更新recentLeader

Server
- 持有一个StateMachine，一个raftNode，一个接收来自raft节点的apply消息的队列applyChan，一个map，存储logindex和其操作结果队列waitChan
- 工作流程如下，以put/append操作为例
    - 接收到rpc，将操作交给raftNode进行log，接收到raftNode是否leader，新log的term和index
    - 在waitChan中以index为键注册一个新的消息队列，未来的log共识结果将会在这里出现
    - raft节点接收到操作后，写入log，同步给各个follower
    - 若达成共识，将log commit，等待apply
    - apply的结果会通过applyChan到达KVServer，KVServer会根据其index分发到某个waitChan
    - KVServer若在超时时间内从waitChan[index]中读到共识成功的结果，就在状态机上执行操作

# 配置/调优参数

```c++
namespace RaftConfig{
// 心跳间隔
duration heartBeatInterv = milliseconds(200);
// 选举间隔时间下限和上限
duration electionIntervLow = milliseconds(1000);
duration electionIntervHigh = milliseconds(1500);

// requestVote RPC 的超时时间
duration requestVoteTimeout = milliseconds(500);

// apply的时间间隔
duration applyInterv = milliseconds(500);
};

namespace ServerConfig {
// Server等待raftNode判断共识的时间
duration consensusTimeout = milliseconds(500);
}
```