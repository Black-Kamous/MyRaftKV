# 问题一：选举的多线程问题
选举线程开启多个线程向所有peer发送rpc，在处理requestVote结果时存在多线程同步的问题。

rpc需要一定时间，因此主线程应使用条件变量的wait来等待rpc完成，但等待的时间不能是无限的，本节点有可能和其他节点网络分区，得到的票数永远不可能过半，这样可能会导致主线程一直在wait中等待。有两种思路可以解决这个问题：

- 选举计时器会在一段时间后重新发起一次选举，届时会更新term（增加1），可以在本线程中保存一个term，在wait时将其与公共的currentTerm比较，若发现自己持有的term已经过时，就无条件终止这一次选举，让位给新的选举流程。
- 给rpc设置一定超时时间，若过了这段时间仍然没有收集到足够的选票就结束这次选举，rpc的超时时间依赖于通信，但应是远小于选举超时时间（范围）的。

# 问题二：锁的优化

- 通常来说，一个对象使用一个互斥量来管理自己的多个线程对自己资源的访问，使用互斥量的个数越多，管理起来越复杂，发生死锁的概率越大。
- 在此基础上，优化加锁和解锁在代码中的位置可以解决死锁问题和优化并发程度。
- 可以用共享互斥量替换互斥量，在特定情景下使用读写锁优化并发程度。如：KVServer的底层状态机使用一个map替代，对这个map的多线程操作可以使用读写锁管理

# 问题三：持久化

- 联动redis持久化


# 问题四： 快照及其优化

- 制作快照和传播快照时如何处理新的数据操作

- 优化：全量快照和增量快照
    - 需要考虑的是传播快照的过程使用了RPC，也就是说会在网络上传输快照信息，如果每次都传播全量快照会带来很大性能损耗
    - 如何得到正确的增量快照？使用增量快照会带来什么问题？

# 优化点：prevote

- 在原先的选举流程中，candidate先自增term，再向其他节点发送requestVote，如果发生网络分区，节点数未过半的分区内会不断发生重新选举，导致term数会推到比正常工作节点高很多的一个量
    - 这样的结果就是，当这些节点恢复到网络中时，不可能成为leader，但比实际leader的term高出很多。
    - 这些恢复节点会拒绝来自leader的复制日志RPC（因为自己的term更大）。在存在能够正常运行的leader的情况下，这些恢复节点会长期无法正常工作（保存正确的数据副本），即使不存在能正常运行的leader，实际具备leader能力的节点会在整个网络中发起选举，而他也只能在反复发起多轮选举，将term推到和网络分区恢复节点相等时才能获得leader
地位，这会无效拉长选举过程

- 解决办法PreVote：很简单的优化，调换升级为leader和自增term的顺序
    - 先向所有节点发送requestVote，并声称自己的term为term+1，但并不实际自增，同时，也不向自己投票
    - 如果获取了多数人-1（除去自己）的票，继续流程，否则不改变任何状态
    - 获得足够票数后，投票给自己，成为leader，并自增term

# 优化点：no-op

- Raft有一个重要的原则：leader只能通过多数共识提交当前任期的log，不能通过多数共识提交之前任期的log
    - 进一步解释：有两种情况能提交（commit）一条log：1. 一条新产生的log在复制到多数节点后，在leader节点提交 2. 新的log提交成功时会累计提交它之前所有未提交的log
    - 该原则限制leader非当前任期的log只能通过情况2提交，不能通过1
    - 这个原则保护了共识原则，反例见论文Figure 8

- 遵守该原则可能会出现一点性能损耗：
    - 新leader当选后，必须等到新的log产生或下一次心跳，才能继续下面的流程，如果没有新log产生，那么它之前的log就无法得到commit
    - 因此，在新leader当选时立即产生一个当前任期没有任何操作的log，既满足了前面的原则，有不会产生额外延迟


# 问题五：幽灵复现

问题见 https://developer.aliyun.com/group/alitech/article/#/

Raft通过上面的只提交最新term log和投票条件的限制（只投给最新term和最长log的节点）解决了幽灵复现问题

# 优化点：ReadIndex

- 首先思考一种很自然的强一致性读方法：LogRead，读操作虽然不改变数据库内容，但是我们也为其创造一条日志，通过心跳RPC传播到各个follower节点，获得多数共识后再执行读取，将结果返回给用户
    - 读取也需要过共识这一关，这是必要的，一个很简单的反例：当前leader发生了网络分区，实际上失去了leader资格，但在发生超时选举之前它无法得知这件事，此时如果遇到读请求而不去尝试获取共识，就会发生一致性错误，因为此时leader可能另有其人，而且新leader上已经有新的操作了。
    - LogRead实际上也保证了读操作的线性一致性
    - 在此基础上，可以优化获取共识的过程，尽可能避免RPC传输log和log落盘等性能损耗

- ReadIndex优化省去了传播读log的过程，只使用心跳，如果收到过半节点的响应就直接在状态机执行读取，为了保证线性一致，记录收到读请求时的commitIndex，因此该优化称为ReadIndex
    - 收到读请求时记录当前的commitIndex，作为readindex
    - 执行一轮心跳，如果收到过半节点响应，可以确定自己当前的leader身份
    - 直到自己的applyIndex大于等于readindex，就可以执行读操作，将结果返回给客户端
    - 当一个Leader刚上任时，有可能存在之前任期未提交的log，这个现象前面已经详细介绍过，此时直接执行ReadIndex的话，由于没有新log产生，需要等到下一个当前任期新log的产生才能推进读流程，会增加操作延迟，好在前面已经进行过了no-op优化，leader可以在上任后快速进入可ReadIndex状态，这两个优化是相得益彰的。


# 优化点：非Leader读取

- 很容易想到，直接在Follower上执行读取是牺牲一致性换取性能，优化点在于：如何让Follower保证一致性？
    - 默认的行为是Follower将自己得知的Leader节点告诉客户，让客户去问Leader
    - 如果让Follower将请求转发给Leader，代价似乎并没减少，还可能增加并发量
    - 在ReadIndex优化后，Follower可以在本地执行读，因为读并不产生日志，也就不需要和其他节点同步
    - 但仍然需要保证对读操作的共识和线性一致性，这里Follower只需要向leader请求一个readindex就可以了


# 优化点：线程池

- 联动线程池项目
